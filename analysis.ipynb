{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39892506",
   "metadata": {},
   "source": [
    "## Phase 1: Code for Data Loading, Cleaning & Feature Engineering\n",
    "### Part 1: Setup and Initial Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5a0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42958 entries, 0 to 42957\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Listing_ID                 42958 non-null  int64  \n",
      " 1   Price                      42958 non-null  int64  \n",
      " 2   Agency_Name                42958 non-null  object \n",
      " 3   Postcode                   42958 non-null  int64  \n",
      " 4   Address                    42958 non-null  object \n",
      " 5   Suburb                     42958 non-null  object \n",
      " 6   Longitude                  42958 non-null  float64\n",
      " 7   Latitude                   42958 non-null  float64\n",
      " 8   Property_Type              42958 non-null  object \n",
      " 9   Bedrooms                   42958 non-null  int64  \n",
      " 10  Bathrooms                  42958 non-null  int64  \n",
      " 11  Parking_Spaces             42958 non-null  int64  \n",
      " 12  Date_Sold                  42958 non-null  object \n",
      " 13  Land_Size                  42958 non-null  int64  \n",
      " 14  Primary_School_Name        42958 non-null  object \n",
      " 15  Primary_School_Distance    42958 non-null  int64  \n",
      " 16  Primary_School_ICSEA       42958 non-null  int64  \n",
      " 17  Secondary_School_Name      42958 non-null  object \n",
      " 18  Secondary_School_Distance  42958 non-null  int64  \n",
      " 19  Secondary_School_ICSEA     42958 non-null  int64  \n",
      " 20  Distance_to_CBD            42958 non-null  int64  \n",
      "dtypes: float64(2), int64(12), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. SETUP & INITIAL DATA LOADING\n",
    "# ---------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8) # Set default figure size\n",
    "\n",
    "# Load the dataset from the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('perth_property_data.csv')\n",
    "\n",
    "# --- Initial Inspection ---\n",
    "\n",
    "# Display a concise summary of the DataFrame.\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda334ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics:\n",
      "         Listing_ID         Price      Postcode     Longitude      Latitude  \\\n",
      "count  4.295800e+04  4.295800e+04  42958.000000  42958.000000  42958.000000   \n",
      "mean   1.391644e+08  8.610860e+05   6083.335840    115.867264    -31.956828   \n",
      "std    6.312142e+06  5.956430e+05     54.925532      0.077023      0.087968   \n",
      "min    1.014242e+08  1.000000e+00   6003.000000    115.732150    -32.133780   \n",
      "25%    1.375409e+08  5.100000e+05   6026.000000    115.804930    -32.034580   \n",
      "50%    1.409794e+08  6.875000e+05   6064.000000    115.854050    -31.948990   \n",
      "75%    1.432736e+08  9.800000e+05   6149.000000    115.923790    -31.886392   \n",
      "max    1.457244e+08  6.350000e+06   6166.000000    116.064601    -31.774076   \n",
      "\n",
      "           Bedrooms     Bathrooms  Parking_Spaces     Land_Size  \\\n",
      "count  42958.000000  42958.000000    42958.000000  4.295800e+04   \n",
      "mean       3.366614      1.731831        2.039643  4.179946e+03   \n",
      "std        0.878614      0.643763        1.129544  7.392181e+05   \n",
      "min        1.000000      1.000000        0.000000  2.900000e+01   \n",
      "25%        3.000000      1.000000        1.000000  3.100000e+02   \n",
      "50%        3.000000      2.000000        2.000000  5.070000e+02   \n",
      "75%        4.000000      2.000000        2.000000  7.150000e+02   \n",
      "max        9.000000      7.000000       12.000000  1.531600e+08   \n",
      "\n",
      "       Primary_School_Distance  Primary_School_ICSEA  \\\n",
      "count             42958.000000          42958.000000   \n",
      "mean                677.729620           1060.287350   \n",
      "std                 392.793154             78.267414   \n",
      "min                  15.000000            667.000000   \n",
      "25%                 408.000000           1019.000000   \n",
      "50%                 605.000000           1073.000000   \n",
      "75%                 855.000000           1119.000000   \n",
      "max                3243.000000           1197.000000   \n",
      "\n",
      "       Secondary_School_Distance  Secondary_School_ICSEA  Distance_to_CBD  \n",
      "count               42958.000000            42958.000000     42958.000000  \n",
      "mean                 1298.668956             1040.386610     11236.235905  \n",
      "std                   738.641623               87.918968      4794.768489  \n",
      "min                    32.000000              696.000000       678.000000  \n",
      "25%                   780.000000              979.000000      7540.000000  \n",
      "50%                  1174.000000             1036.000000     11257.000000  \n",
      "75%                  1643.000000             1109.000000     15002.750000  \n",
      "max                  4735.000000             1239.000000     20000.000000  \n"
     ]
    }
   ],
   "source": [
    "# Generate descriptive statistics for numerical columns.\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d24adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded 10 rules from corrections.csv.\n",
      "Deleted 4 rows based on DELETE_ROW rules.\n",
      "Updated Listing_ID 138928707: Set column 'Price' to 730000.\n",
      "Updated Listing_ID 131703414: Set column 'Price' to 750000.\n",
      "Updated Listing_ID 142122104: Set column 'Price' to 530000.\n",
      "Updated Listing_ID 142973724: Set column 'Land_Size' to 404.\n",
      "Updated Listing_ID 105535616: Set column 'Postcode' to 6152.\n",
      "Updated Listing_ID 104755052: Set column 'Postcode' to 6014.\n",
      "\n",
      "Data ready for automated cleaning. Current rows: 42954.\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Apply Manual Corrections ---\n",
    "df_raw = pd.read_csv('perth_property_data.csv')\n",
    "try:\n",
    "    # Load the corrections file\n",
    "    df_corrections = pd.read_csv('corrections.csv')\n",
    "    print(f\"Found and loaded {len(df_corrections)} rules from corrections.csv.\")\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Step 1: Handle Deletions\n",
    "    # Identify all Listing_IDs marked for deletion.\n",
    "    ids_to_delete = df_corrections[df_corrections['New_Value'] == 'DELETE_ROW']['Listing_ID'].astype(int).tolist()\n",
    "    \n",
    "    if ids_to_delete:\n",
    "        initial_rows = len(df)\n",
    "        # Use .isin() to filter out the rows to be deleted. The ~ inverts the selection.\n",
    "        df = df[~df['Listing_ID'].isin(ids_to_delete)]\n",
    "        print(f\"Deleted {initial_rows - len(df)} rows based on DELETE_ROW rules.\")\n",
    "    \n",
    "    # Step 2: Handle Value Updates\n",
    "    # Filter for rules that are not deletions.\n",
    "    corrections_to_apply = df_corrections[df_corrections['New_Value'] != 'DELETE_ROW'].copy()\n",
    "    # Ensure data types are correct for matching\n",
    "    corrections_to_apply['Listing_ID'] = corrections_to_apply['Listing_ID'].astype(int)\n",
    "    corrections_to_apply['New_Value'] = pd.to_numeric(corrections_to_apply['New_Value'])\n",
    "\n",
    "    # Iterate through each correction rule and apply it using .loc for precision.\n",
    "    for index, rule in corrections_to_apply.iterrows():\n",
    "        listing_id = rule['Listing_ID']\n",
    "        column = rule['Column_To_Correct']\n",
    "        new_value = rule['New_Value']\n",
    "        \n",
    "        # This command finds the exact row(s) and column and sets the new value.\n",
    "        df.loc[df['Listing_ID'] == listing_id, column] = new_value\n",
    "        print(f\"Updated Listing_ID {listing_id}: Set column '{column}' to {new_value}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: corrections.csv not found. Proceeding with raw data.\")\n",
    "    df = df_raw.copy()\n",
    "\n",
    "# --- Manual Correction Complete ---\n",
    "# All subsequent cleaning and analysis will be performed on the 'df' DataFrame.\n",
    "print(f\"\\nData ready for automated cleaning. Current rows: {len(df)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9da2060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting technical cleaning...\n",
      "Successfully converted 'Date_Sold' to datetime objects.\n",
      "Engineered new features: 'Sale_Year', 'Sale_Month', 'Sale_DayOfWeek'.\n",
      "\n",
      "--- Final DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42954 entries, 0 to 42954\n",
      "Data columns (total 24 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   Listing_ID                 42954 non-null  int64         \n",
      " 1   Price                      42954 non-null  int64         \n",
      " 2   Agency_Name                42954 non-null  object        \n",
      " 3   Postcode                   42954 non-null  int64         \n",
      " 4   Address                    42954 non-null  object        \n",
      " 5   Suburb                     42954 non-null  object        \n",
      " 6   Longitude                  42954 non-null  float64       \n",
      " 7   Latitude                   42954 non-null  float64       \n",
      " 8   Property_Type              42954 non-null  object        \n",
      " 9   Bedrooms                   42954 non-null  int64         \n",
      " 10  Bathrooms                  42954 non-null  int64         \n",
      " 11  Parking_Spaces             42954 non-null  int64         \n",
      " 12  Date_Sold                  42954 non-null  datetime64[ns]\n",
      " 13  Land_Size                  42954 non-null  int64         \n",
      " 14  Primary_School_Name        42954 non-null  object        \n",
      " 15  Primary_School_Distance    42954 non-null  int64         \n",
      " 16  Primary_School_ICSEA       42954 non-null  int64         \n",
      " 17  Secondary_School_Name      42954 non-null  object        \n",
      " 18  Secondary_School_Distance  42954 non-null  int64         \n",
      " 19  Secondary_School_ICSEA     42954 non-null  int64         \n",
      " 20  Distance_to_CBD            42954 non-null  int64         \n",
      " 21  Sale_Year                  42954 non-null  int32         \n",
      " 22  Sale_Month                 42954 non-null  int32         \n",
      " 23  Sale_DayOfWeek             42954 non-null  int32         \n",
      "dtypes: datetime64[ns](1), float64(2), int32(3), int64(12), object(6)\n",
      "memory usage: 7.7+ MB\n",
      "\n",
      "--- Final Descriptive Statistics ---\n",
      "         Listing_ID         Price      Postcode     Longitude      Latitude  \\\n",
      "count  4.295400e+04  4.295400e+04  42954.000000  42954.000000  42954.000000   \n",
      "mean   1.391665e+08  8.611739e+05   6083.332053    115.867260    -31.956824   \n",
      "min    1.014242e+08  1.000000e+05   6003.000000    115.732150    -32.133780   \n",
      "25%    1.375440e+08  5.100000e+05   6026.000000    115.804930    -32.034580   \n",
      "50%    1.409796e+08  6.875000e+05   6064.000000    115.854026    -31.948990   \n",
      "75%    1.432742e+08  9.800000e+05   6149.000000    115.923790    -31.886392   \n",
      "max    1.457244e+08  6.350000e+06   6166.000000    116.064601    -31.774076   \n",
      "std    6.307984e+06  5.955753e+05     54.925397      0.077024      0.087963   \n",
      "\n",
      "           Bedrooms     Bathrooms  Parking_Spaces  \\\n",
      "count  42954.000000  42954.000000    42954.000000   \n",
      "mean       3.366555      1.731806        2.039554   \n",
      "min        1.000000      1.000000        0.000000   \n",
      "25%        3.000000      1.000000        1.000000   \n",
      "50%        3.000000      2.000000        2.000000   \n",
      "75%        4.000000      2.000000        2.000000   \n",
      "max        9.000000      7.000000       12.000000   \n",
      "std        0.878607      0.643751        1.129187   \n",
      "\n",
      "                           Date_Sold     Land_Size  Primary_School_Distance  \\\n",
      "count                          42954  42954.000000             42954.000000   \n",
      "mean   2022-06-23 16:23:52.155329024    519.875355               677.651837   \n",
      "min              2006-06-13 00:00:00     29.000000                15.000000   \n",
      "25%              2021-12-10 00:00:00    310.000000               408.000000   \n",
      "50%              2023-02-14 00:00:00    507.000000               605.000000   \n",
      "75%              2023-11-15 00:00:00    715.000000               855.000000   \n",
      "max              2024-08-09 00:00:00   1533.000000              3243.000000   \n",
      "std                              NaN    262.761673               392.699566   \n",
      "\n",
      "       Primary_School_ICSEA  Secondary_School_Distance  \\\n",
      "count          42954.000000               42954.000000   \n",
      "mean            1060.291358                1298.614355   \n",
      "min              667.000000                  32.000000   \n",
      "25%             1019.000000                 780.000000   \n",
      "50%             1073.000000                1174.000000   \n",
      "75%             1119.000000                1643.000000   \n",
      "max             1197.000000                4735.000000   \n",
      "std               78.269032                 738.564301   \n",
      "\n",
      "       Secondary_School_ICSEA  Distance_to_CBD     Sale_Year    Sale_Month  \\\n",
      "count            42954.000000     42954.000000  42954.000000  42954.000000   \n",
      "mean              1040.395493     11235.797178   2021.986171      6.417749   \n",
      "min                723.000000       678.000000   2006.000000      1.000000   \n",
      "25%                979.000000      7540.000000   2021.000000      4.000000   \n",
      "50%               1036.000000     11257.000000   2023.000000      6.000000   \n",
      "75%               1109.000000     15001.000000   2023.000000      9.000000   \n",
      "max               1239.000000     20000.000000   2024.000000     12.000000   \n",
      "std                 87.907244      4794.515182      2.239173      3.406264   \n",
      "\n",
      "       Sale_DayOfWeek  \n",
      "count     42954.00000  \n",
      "mean          2.24035  \n",
      "min           0.00000  \n",
      "25%           1.00000  \n",
      "50%           2.00000  \n",
      "75%           4.00000  \n",
      "max           6.00000  \n",
      "std           1.66540  \n",
      "\n",
      "Technical cleaning and feature engineering complete. The dataset is ready for EDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_21419/4092186627.py:19: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Date_Sold'] = pd.to_datetime(df['Date_Sold'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2. TECHNICAL CLEANING & FEATURE ENGINEERING\n",
    "#\n",
    "# Decision: Based on data validation, the extreme values for both 'Price' and \n",
    "# 'Land_Size' are considered legitimate and will be retained to ensure a\n",
    "# complete market representation. Automated outlier removal will be skipped.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "key_text_columns = ['Agency_Name', 'Primary_School_Name', 'Secondary_School_Name']\n",
    "\n",
    "for col in key_text_columns:\n",
    "    # Check if the column exists and is of object type before applying string operations.\n",
    "    if col in df.columns and df[col].dtype == 'object':\n",
    "        df.loc[:, col] = df[col].str.strip().str.lower()\n",
    "        \n",
    "# convert 'Date_Sold' column from an object (text) to a datetime object.\n",
    "\n",
    "print(\"Starting technical cleaning...\")\n",
    "df['Date_Sold'] = pd.to_datetime(df['Date_Sold'], errors='coerce')\n",
    "\n",
    "# Optional but recommended: Check for and handle any conversion errors.\n",
    "if df['Date_Sold'].isnull().any():\n",
    "    num_errors = df['Date_Sold'].isnull().sum()\n",
    "    print(f\"Warning: Found {num_errors} date(s) that could not be parsed. These rows will be dropped.\")\n",
    "    df.dropna(subset=['Date_Sold'], inplace=True)\n",
    "\n",
    "print(\"Successfully converted 'Date_Sold' to datetime objects.\")\n",
    "\n",
    "\n",
    "# --- 2.2 Feature Engineering ---\n",
    "# This step creates new, valuable features from existing data without altering it.\n",
    "# We extract time-based features from the 'Date_Sold' column.\n",
    "\n",
    "df['Sale_Year'] = df['Date_Sold'].dt.year\n",
    "df['Sale_Month'] = df['Date_Sold'].dt.month\n",
    "df['Sale_DayOfWeek'] = df['Date_Sold'].dt.dayofweek # Note: Monday=0, Sunday=6\n",
    "\n",
    "print(\"Engineered new features: 'Sale_Year', 'Sale_Month', 'Sale_DayOfWeek'.\")\n",
    "\n",
    "\n",
    "# --- 2.3 Final Verification ---\n",
    "# A final check to ensure our DataFrame is ready for analysis.\n",
    "\n",
    "# We use .copy() here to create a definitive 'cleaned' version of the DataFrame.\n",
    "# This can help prevent 'SettingWithCopyWarning' in later exploratory stages.\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "print(\"\\n--- Final DataFrame Info ---\")\n",
    "df_cleaned.info()\n",
    "\n",
    "print(\"\\n--- Final Descriptive Statistics ---\")\n",
    "print(df_cleaned.describe())\n",
    "\n",
    "print(\"\\nTechnical cleaning and feature engineering complete. The dataset is ready for EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a130a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered new feature: 'Layout' (e.g., 3b2b, 4b2b).\n",
      "\n",
      "Top 15 most common property layouts:\n",
      "Layout\n",
      "4b2b    12548\n",
      "3b1b     9622\n",
      "3b2b     8604\n",
      "2b1b     3245\n",
      "4b3b     1639\n",
      "4b1b     1580\n",
      "5b2b     1413\n",
      "2b2b     1318\n",
      "1b1b     1007\n",
      "5b3b      907\n",
      "3b3b      242\n",
      "5b4b      171\n",
      "6b3b      158\n",
      "4b4b      127\n",
      "6b2b      113\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new categorical feature by combining bedrooms and bathrooms.\n",
    "# This captures the \"layout\" or \"floor plan\" of the property as a single feature.\n",
    "# We convert them to string to concatenate them.\n",
    "df['Layout'] = df['Bedrooms'].astype(str) + 'b' + df['Bathrooms'].astype(str) + 'b'\n",
    "\n",
    "print(\"Engineered new feature: 'Layout' (e.g., 3b2b, 4b2b).\")\n",
    "\n",
    "# Let's inspect the most common layouts\n",
    "print(\"\\nTop 15 most common property layouts:\")\n",
    "print(df['Layout'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efea311",
   "metadata": {},
   "source": [
    "### Perth Property Data from CSV to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68016d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Setup and Configuration ---\n",
      "Successfully connected to the MySQL database!\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 1: Setup and Configuration ---\n",
    "print(\"--- Phase 1: Setup and Configuration ---\")\n",
    "\n",
    "# --- Step 1: Import necessary libraries ---\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "\n",
    "# --- Step 2: Database connection credentials ---\n",
    "DB_USER = 'root'\n",
    "DB_PASS = 'password' \n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '3306'\n",
    "DB_NAME = 'perth_property_db' \n",
    "\n",
    "# --- Step 3: Create the SQLAlchemy engine ---\n",
    "# This engine acts as the central point of contact between our script and the database.\n",
    "try:\n",
    "    db_connection_str = f'mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "    engine = create_engine(db_connection_str)\n",
    "    \n",
    "    # --- Step 4: Test the connection ---\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Successfully connected to the MySQL database!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not connect to the database. Please check your credentials and ensure MySQL is running. \\n{e}\")\n",
    "    # We stop the script here if connection fails.\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db3832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Dim DataFrames ---\n",
      "Created DIM_Layouts DataFrame with 39 unique layouts.\n",
      "Prepared DIM_Suburbs with 203 unique suburbs.\n",
      "Prepared DIM_Agencies with 861 unique agencies.\n",
      "Created DIM_Primary_Schools DataFrame with 344 unique primary schools.\n",
      "Created DIM_Secondary_Schools DataFrame with 123 unique secondary schools.\n"
     ]
    }
   ],
   "source": [
    "## use the df we cleaned before \n",
    "df_cleaned = df.copy()\n",
    "# --- Prepare Dimension DataFrames ---\n",
    "print(\"\\n--- Preparing Dim DataFrames ---\")\n",
    "\n",
    "# --- Dimension 1: Layouts ---\n",
    "# The combination of bedrooms and bathrooms is already unique by definition.\n",
    "df_layouts = df_cleaned[['Bedrooms', 'Bathrooms', 'Layout']].drop_duplicates().reset_index(drop=True)\n",
    "df_layouts = df_layouts.rename(columns={'Bedrooms': 'bedrooms', 'Bathrooms': 'bathrooms', 'Layout': 'layout_name'})\n",
    "print(f\"Created DIM_Layouts DataFrame with {len(df_layouts)} unique layouts.\")\n",
    "\n",
    "# Dimension 2: Suburbs\n",
    "# CORRECTED: Drop duplicates based *only* on the 'Suburb' column to match the UNIQUE constraint in SQL.\n",
    "df_suburbs = df_cleaned[['Suburb', 'Postcode']].drop_duplicates().reset_index(drop=True).rename(columns={'Suburb': 'suburb_name', 'Postcode': 'postcode'})\n",
    "print(f\"Prepared DIM_Suburbs with {len(df_suburbs)} unique suburbs.\")\n",
    "# Dimension 3: Agencies\n",
    "df_agencies = pd.DataFrame(df_cleaned['Agency_Name'].dropna().unique(), columns=['agency_name'])\n",
    "print(f\"Prepared DIM_Agencies with {len(df_agencies)} unique agencies.\")\n",
    "\n",
    "# --- Dimension 4: Primary Schools ---\n",
    "# CORRECTED: Explicitly drop duplicates based on the school name.\n",
    "df_primary_schools = df_cleaned[['Primary_School_Name', 'Primary_School_ICSEA']].dropna(subset=['Primary_School_Name']).drop_duplicates(subset=['Primary_School_Name']).reset_index(drop=True)\n",
    "df_primary_schools = df_primary_schools.rename(columns={'Primary_School_Name': 'primary_school_name', 'Primary_School_ICSEA': 'primary_school_icsea'})\n",
    "print(f\"Created DIM_Primary_Schools DataFrame with {len(df_primary_schools)} unique primary schools.\")\n",
    "\n",
    "# --- Dimension 5: Secondary Schools ---\n",
    "# CORRECTED: Explicitly drop duplicates based on the school name.\n",
    "df_secondary_schools = df_cleaned[['Secondary_School_Name', 'Secondary_School_ICSEA']].dropna(subset=['Secondary_School_Name']).drop_duplicates(subset=['Secondary_School_Name']).reset_index(drop=True)\n",
    "df_secondary_schools = df_secondary_schools.rename(columns={'Secondary_School_Name': 'secondary_school_name', 'Secondary_School_ICSEA': 'secondary_school_icsea'})\n",
    "print(f\"Created DIM_Secondary_Schools DataFrame with {len(df_secondary_schools)} unique secondary schools.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1b21f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Data into MySQL ---\n",
      "Executing create_tables.sql script...\n",
      "Successfully executed create_tables.sql script (All tables dropped and recreated).\n",
      "Loaded 39 records into DIM_Layouts.\n",
      "Loaded 203 records into DIM_Suburbs.\n",
      "Loaded 861 records into DIM_Agencies.\n",
      "Loaded 344 records into DIM_Primary_Schools.\n",
      "Loaded 123 records into DIM_Secondary_Schools.\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3: Load Data into MySQL ---\n",
    "print(\"\\n--- Loading Data into MySQL ---\")\n",
    "\n",
    "try:\n",
    "    # Use a single connection for all operations in this block.\n",
    "    with engine.connect() as connection:\n",
    "        \n",
    "        # --- Step 1: Recreate all tables from the SQL script ---\n",
    "        # This is the corrected section.\n",
    "        print(\"Executing create_tables.sql script...\")\n",
    "        sql_script_path = os.path.join('sql', 'create_tables.sql')\n",
    "        \n",
    "        with open(sql_script_path, 'r') as f:\n",
    "            # Read the entire SQL script into a single string.\n",
    "            sql_script = f.read()\n",
    "            \n",
    "            # Split the script into individual statements using the semicolon as a delimiter.\n",
    "            sql_statements = sql_script.split(';')\n",
    "            \n",
    "            # Execute each statement one by one.\n",
    "            # Begin a transaction to ensure all statements succeed or none do.\n",
    "            with connection.begin():\n",
    "                for statement in sql_statements:\n",
    "                    # We must check if the statement is not just whitespace or comments.\n",
    "                    if statement.strip():\n",
    "                        connection.execute(text(statement))\n",
    "                        \n",
    "        print(\"Successfully executed create_tables.sql script (All tables dropped and recreated).\")\n",
    "\n",
    "        # --- Step 2: Load each dimension DataFrame into the corresponding table ---\n",
    "        # This part remains the same as it correctly loads data table by table.\n",
    "        with connection.begin(): # Use a new transaction for loading data\n",
    "            df_layouts.to_sql('DIM_Layouts', con=connection, if_exists='append', index=False)\n",
    "            print(f\"Loaded {len(df_layouts)} records into DIM_Layouts.\")\n",
    "\n",
    "            df_suburbs.to_sql('DIM_Suburbs', con=connection, if_exists='append', index=False)\n",
    "            print(f\"Loaded {len(df_suburbs)} records into DIM_Suburbs.\")\n",
    "            \n",
    "            df_agencies.to_sql('DIM_Agencies', con=connection, if_exists='append', index=False)\n",
    "            print(f\"Loaded {len(df_agencies)} records into DIM_Agencies.\")\n",
    "\n",
    "            df_primary_schools.to_sql('DIM_Primary_Schools', con=connection, if_exists='append', index=False)\n",
    "            print(f\"Loaded {len(df_primary_schools)} records into DIM_Primary_Schools.\")\n",
    "\n",
    "            df_secondary_schools.to_sql('DIM_Secondary_Schools', con=connection, if_exists='append', index=False)\n",
    "            print(f\"Loaded {len(df_secondary_schools)} records into DIM_Secondary_Schools.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during dimension table loading: \\n{e}\")\n",
    "    # To help with debugging, we re-raise the exception after printing.\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4938e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading the Fact Table ---\n",
      "Fetched dimension tables back from DB to map foreign keys.\n",
      "Foreign keys merged into the main DataFrame.\n",
      "\n",
      "--- DEBUG: Checking for NULLs in foreign key columns ---\n",
      "Count of nulls in each foreign key column after merge:\n",
      "suburb_id              0\n",
      "agency_id              0\n",
      "layout_id              0\n",
      "primary_school_id      0\n",
      "secondary_school_id    0\n",
      "dtype: int64\n",
      "SUCCESS: All foreign keys were merged successfully. No nulls found.\n",
      "\n",
      "Successfully loaded 42954 records into FACT_Properties!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Block 4: Prepare and Load the Fact Table\n",
    "#\n",
    "print(\"\\n Loading the Fact Table ---\")\n",
    "\n",
    "try:\n",
    "    # The outer 'with' statement manages the connection and a transaction.\n",
    "    with engine.connect() as connection:\n",
    "        \n",
    "        # --- Step 1: Fetch dimension tables back from DB to get auto-generated IDs ---\n",
    "        layouts_map = pd.read_sql(\"SELECT layout_id, layout_name FROM DIM_Layouts\", connection)\n",
    "        suburbs_map = pd.read_sql(\"SELECT suburb_id, suburb_name FROM DIM_Suburbs\", connection)\n",
    "        agencies_map = pd.read_sql(\"SELECT agency_id, agency_name FROM DIM_Agencies\", connection)\n",
    "        primary_schools_map = pd.read_sql(\"SELECT primary_school_id, primary_school_name FROM DIM_Primary_Schools\", connection)\n",
    "        secondary_schools_map = pd.read_sql(\"SELECT secondary_school_id, secondary_school_name FROM DIM_Secondary_Schools\", connection)\n",
    "        print(\"Fetched dimension tables back from DB to map foreign keys.\")\n",
    "        \n",
    "        # --- Step 2: Merge foreign keys back into the main DataFrame ---\n",
    "        df_merged = df_cleaned.copy()\n",
    "        df_merged = pd.merge(df_merged, layouts_map, left_on='Layout', right_on='layout_name', how='left')\n",
    "        df_merged = pd.merge(df_merged, suburbs_map, left_on='Suburb', right_on='suburb_name', how='left')\n",
    "        df_merged = pd.merge(df_merged, agencies_map, left_on='Agency_Name', right_on='agency_name', how='left')\n",
    "        df_merged = pd.merge(df_merged, primary_schools_map, left_on='Primary_School_Name', right_on='primary_school_name', how='left')\n",
    "        df_merged = pd.merge(df_merged, secondary_schools_map, left_on='Secondary_School_Name', right_on='secondary_school_name', how='left')\n",
    "        print(\"Foreign keys merged into the main DataFrame.\")\n",
    "\n",
    "        # --- Step 3: Debugging Checkpoint for NULL Foreign Keys ---\n",
    "        print(\"\\n--- DEBUG: Checking for NULLs in foreign key columns ---\")\n",
    "        fk_columns = ['suburb_id', 'agency_id', 'layout_id', 'primary_school_id', 'secondary_school_id']\n",
    "        null_counts = df_merged[fk_columns].isnull().sum()\n",
    "        print(\"Count of nulls in each foreign key column after merge:\")\n",
    "        print(null_counts)\n",
    "        if null_counts.sum() == 0:\n",
    "            print(\"SUCCESS: All foreign keys were merged successfully. No nulls found.\")\n",
    "\n",
    "        # --- Step 4: Prepare the final fact table DataFrame ---\n",
    "        fact_table_columns = [\n",
    "            'Listing_ID', 'Price', 'Address', 'Longitude', 'Latitude', 'Property_Type', 'Parking_Spaces', \n",
    "            'Date_Sold', 'Land_Size', 'Distance_to_CBD', 'Primary_School_Distance', 'Secondary_School_Distance',\n",
    "            'suburb_id', 'agency_id', 'layout_id', 'primary_school_id', 'secondary_school_id'\n",
    "        ]\n",
    "        df_fact = df_merged[fact_table_columns].copy()\n",
    "        \n",
    "        df_fact.rename(columns={\n",
    "            'Listing_ID': 'listing_id', 'Price': 'price', 'Address': 'address', 'Longitude': 'longitude',\n",
    "            'Latitude': 'latitude', 'Property_Type': 'property_type', 'Parking_Spaces': 'parking_spaces',\n",
    "            'Date_Sold': 'date_sold', 'Land_Size': 'land_size', 'Distance_to_CBD': 'distance_to_cbd',\n",
    "            'Primary_School_Distance': 'primary_school_distance', 'Secondary_School_Distance': 'secondary_school_distance'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        initial_rows = len(df_fact)\n",
    "        df_fact.dropna(subset=fk_columns, inplace=True)\n",
    "        rows_dropped = initial_rows - len(df_fact)\n",
    "        if rows_dropped > 0:\n",
    "            print(f\"Dropped {rows_dropped} rows due to null foreign keys before loading.\")\n",
    "        \n",
    "        # --- Step 5: Load the fact table into the database ---\n",
    "        # The 'to_sql' call is now directly inside the main 'with' block.\n",
    "        # This block will automatically commit on success or rollback on error.\n",
    "        df_fact.to_sql('FACT_Properties', con=connection, if_exists='append', index=False)\n",
    "        # --- CRITICAL FIX: Explicitly commit the transaction after loading the fact table ---\n",
    "        connection.commit()\n",
    "        print(f\"\\nSuccessfully loaded {len(df_fact)} records into FACT_Properties!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during fact table loading process: \\n{e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
